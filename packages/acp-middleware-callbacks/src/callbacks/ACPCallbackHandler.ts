/**
 * ACP Callback Handler
 * 
 * Callback handler for emitting events to ACP clients during agent execution.
 * Handles LLM token streaming and tool call lifecycle events.
 * 
 * @packageDocumentation
 */

import { BaseCallbackHandler } from "@langchain/core/callbacks/base";
import type { 
  ContentBlock,
  TextContent,
  ContentChunk,
} from "../types/acp.js";
import type { ContentBlockMapper, DefaultContentBlockMapper } from "../utils/contentBlockMapper.js";
import { defaultContentBlockMapper } from "../utils/contentBlockMapper.js";
import type { ACPCallbackHandlerConfig, ACPConnection } from "../types/middleware.js";
import { mapLangChainError, ACP_ERROR_CODES } from "../utils/index.js";

/**
 * Callback handler for ACP protocol streaming events.
 * Extends BaseCallbackHandler to integrate with LangChain's callback system
 * and emit events to ACP-compatible clients.
 * 
 * This handler manages:
 * - LLM token streaming via handleLLMNewToken → agent_message_chunk
 * - Reasoning content via handleLLMNewToken → agent_thought_chunk (with audience: ['assistant'])
 * - Tool lifecycle events: start/end/error → tool_call/tool_call_update
 * - State snapshot updates for real-time agent communication
 */
export class ACPCallbackHandler extends BaseCallbackHandler {
  name = "acp-callback-handler";
  
  protected connection: ACPConnection;
  protected contentBlockMapper: ContentBlockMapper;
  protected emitTextChunks: boolean;
  protected includeIntermediateStates: boolean;
  protected maxMessagesInSnapshot: number;
  protected emitReasoningAsThought: boolean;
  
  private currentMessageId: string | null = null;
  private currentTextContent: string = "";
  private currentToolCallId: string | null = null;
  private sessionId: string | null = null;
  
  // Reasoning content tracking
  private currentThoughtMessageId: string | null = null;
  private currentReasoningContent: string = "";
  private isInReasoningBlock: boolean = false;
  private reasoningBlockStartToken: string | null = null;

  constructor(config: ACPCallbackHandlerConfig) {
    super();
    
    this.connection = config.connection;
    this.sessionId = config.sessionId ?? null;
    this.contentBlockMapper = config.contentBlockMapper ?? defaultContentBlockMapper;
    this.emitTextChunks = config.emitTextChunks ?? false;
    this.includeIntermediateStates = config.includeIntermediateStates ?? true;
    this.maxMessagesInSnapshot = config.maxMessagesInSnapshot ?? 50;
    // Whether to emit reasoning content as agent_thought_chunk (unstable protocol feature)
    // Falls back to agent_message_chunk if false
    this.emitReasoningAsThought = config.emitReasoningAsThought ?? true;
  }

  /**
   * Dispose of the callback handler and close the connection.
   */
  async dispose(): Promise<void> {
    try {
      await this.connection.close();
    } catch {
      // Fail-safe: don't let close errors break disposal
    }
  }

  /**
   * Sets the session ID for this callback handler.
   * Should be called by the session middleware before agent execution.
   * 
   * @param sessionId - The session ID to use for session updates
   */
  setSessionId(sessionId: string): void {
    this.sessionId = sessionId;
  }

  /**
   * Gets the current session ID.
   */
  getSessionId(): string | null {
    return this.sessionId;
  }

  /**
   * Called when an LLM starts processing.
   * 
   * @param _llm - The LLM being used
   * @param _prompts - The prompts sent to the LLM
   * @param runId - The run ID for this LLM call
   * @param _parentRunId - The parent run ID if this is a nested call
   * @param _extraParams - Additional parameters
   * @param _tags - Optional tags for this LLM call
   * @param _metadata - Optional metadata for this LLM call
   * @param _runName - Optional run name
   */
  override async handleLLMStart(
    _llm: any,
    _prompts: string[],
    runId: string,
    _parentRunId?: string,
    _extraParams?: Record<string, unknown>,
    _tags?: string[],
    _metadata?: Record<string, unknown>,
    _runName?: string
  ): Promise<void> {
    // Generate a new message ID for this LLM response
    this.currentMessageId = this.generateMessageId();
    this.currentTextContent = "";
    
    // Reset reasoning state
    this.currentThoughtMessageId = null;
    this.currentReasoningContent = "";
    this.isInReasoningBlock = false;
    this.reasoningBlockStartToken = null;
  }

  /**
   * Called when a new token is generated by the LLM.
   * This is the primary method for streaming LLM output to ACP clients.
   * 
   * For LangChain v1.0.0, this handles:
   * - Regular text tokens → agent_message_chunk
   * - Reasoning content blocks → agent_thought_chunk (with audience: ['assistant'])
   * - Tool call chunks → tool_call/tool_call_update
   * 
   * @param token - The new token generated by the LLM
   * @param _idx - Token index information
   * @param runId - The run ID for this LLM call
   * @param _parentRunId - The parent run ID if this is a nested call
   * @param _tags - Optional tags
   * @param fields - Additional fields that may contain chunk/content block data
   */
  override async handleLLMNewToken(
    token: string,
    _idx: any,
    runId: string,
    _parentRunId?: string,
    _tags?: string[],
    fields?: any
  ): Promise<void> {
    if (!this.currentMessageId) {
      this.currentMessageId = this.generateMessageId();
    }
    
    // Check for LangChain v1.0.0 structured content blocks in the chunk
    // The fields parameter may contain chunk data with content blocks
    const chunkContent = this.extractChunkContent(fields);
    
    if (chunkContent) {
      // We have structured content from LangChain v1.0.0
      await this.processChunkContent(chunkContent, fields);
      return;
    }
    
    // Fallback: Check if this token is part of a reasoning block
    // This handles streaming from providers that don't expose content blocks
    const isReasoningToken = this.isReasoningToken(token, fields);
    
    if (isReasoningToken && !this.isInReasoningBlock) {
      // Start of reasoning block
      this.isInReasoningBlock = true;
      this.reasoningBlockStartToken = token;
      this.currentReasoningContent = token;
      this.currentThoughtMessageId = this.generateMessageId();
      
      await this.sendAgentThoughtChunk(
        this.currentThoughtMessageId,
        token,
        token
      );
      return;
    }
    
    if (this.isInReasoningBlock) {
      // Check if this token ends the reasoning block
      if (this.isReasoningEndToken(token, fields)) {
        await this.sendAgentThoughtChunk(
          this.currentThoughtMessageId!,
          token,
          this.currentReasoningContent
        );
        
        this.isInReasoningBlock = false;
        this.currentReasoningContent = "";
        this.currentThoughtMessageId = null;
        this.reasoningBlockStartToken = null;
        return;
      }
      
      // Continue accumulating reasoning content
      this.currentReasoningContent += token;
      
      await this.sendAgentThoughtChunk(
        this.currentThoughtMessageId!,
        token,
        this.currentReasoningContent
      );
      return;
    }
    
    // Regular text content
    this.currentTextContent += token;

    try {
      await this.sendAgentMessageChunk(
        this.currentMessageId,
        token,
        this.currentTextContent
      );
    } catch {
      // Fail-safe: don't let emit errors break agent execution
    }
  }
  
  /**
   * Extracts content from a streaming chunk.
   * LangChain v1.0.0 provides content blocks in various formats during streaming.
   * 
   * @param fields - The fields from handleLLMNewToken
   * @returns Extracted content block or null
   */
  private extractChunkContent(fields?: any): { type: string; reasoning?: string; text?: string } | null {
    if (!fields) return null;
    
    // Check for direct content block in chunk
    if (fields.content && Array.isArray(fields.content)) {
      const blocks = fields.content.filter((b: any) => b && typeof b === 'object' && b.type);
      if (blocks.length > 0) {
        // Return first content block
        return blocks[0];
      }
    }
    
    // Check for chunk property (some providers use this)
    if (fields.chunk && typeof fields.chunk === 'object') {
      const chunk = fields.chunk;
      
      // Check for reasoning content
      if (chunk.reasoning || (chunk.content && Array.isArray(chunk.content))) {
        // If chunk has reasoning property, treat as reasoning block
        if (chunk.reasoning) {
          return { type: "reasoning", reasoning: chunk.reasoning };
        }
        
        // Check content blocks in chunk
        if (Array.isArray(chunk.content)) {
          const blocks = chunk.content.filter((b: any) => b && typeof b === 'object' && b.type);
          if (blocks.length > 0) {
            return blocks[0];
          }
        }
      }
      
      // Check for text content
      if (chunk.text || chunk.content?.text) {
        return { type: "text", text: chunk.text || chunk.content?.text };
      }
    }
    
    // Check for explicit reasoning flag
    if (fields.reasoning === true && fields.text) {
      return { type: "reasoning", reasoning: fields.text };
    }
    
    return null;
  }
  
  /**
   * Processes content from a streaming chunk.
   * Emits reasoning blocks as agent_thought_chunk.
   * 
   * @param content - The content block to process
   * @param fields - Full fields from the callback
   */
  private async processChunkContent(
    content: { type: string; reasoning?: string; text?: string },
    fields?: any
  ): Promise<void> {
    if (content.type === "reasoning" && content.reasoning) {
      // Emit as agent_thought_chunk
      const thoughtMessageId = this.generateMessageId();
      await this.sendAgentThoughtChunk(
        thoughtMessageId,
        content.reasoning,
        content.reasoning
      );
    } else if (content.type === "text" && content.text) {
      // Emit as agent_message_chunk
      this.currentTextContent += content.text;
      
      await this.sendAgentMessageChunk(
        this.currentMessageId!,
        content.text,
        this.currentTextContent
      );
    }
  }

  /**
   * Called when an LLM call ends.
   * 
   * @param output - The output from the LLM (may contain content blocks with reasoning)
   * @param runId - The run ID for this LLM call
   * @param _parentRunId - The parent run ID if this is a nested call
   * @param _tags - Optional tags
   * @param _extraParams - Additional parameters
   */
  override async handleLLMEnd(
    output: any,
    runId: string,
    _parentRunId?: string,
    _tags?: string[],
    _extraParams?: Record<string, unknown>
  ): Promise<void> {
    // Finalize any pending reasoning content from streaming
    if (this.isInReasoningBlock && this.currentThoughtMessageId) {
      try {
        await this.sendAgentThoughtChunk(
          this.currentThoughtMessageId,
          "",
          this.currentReasoningContent
        );
      } catch {
        // Fail-safe: don't let emit errors break agent execution
      }
      
      this.isInReasoningBlock = false;
      this.currentReasoningContent = "";
      this.currentThoughtMessageId = null;
      this.reasoningBlockStartToken = null;
    }
    
    // Check for LangChain v1.0.0 structured content blocks
    // LangChain AIMessageChunk can have content as an array of content blocks
    // including { type: "reasoning", reasoning: string }
    const contentBlocks = this.extractContentBlocks(output);
    
    if (contentBlocks.length > 0) {
      // Process structured content blocks
      await this.processContentBlocks(contentBlocks, this.currentMessageId);
      
      this.currentMessageId = null;
      this.currentTextContent = "";
    } else if (this.currentMessageId) {
      // Fallback to legacy text content handling
      try {
        // Send final agent message with complete content
        const content = this.currentTextContent;
        const textContent: TextContent & { type: "text" } = {
          type: "text",
          text: content,
          _meta: null,
          annotations: null,
        };
        
        await this.connection.sendAgentMessage({
          messageId: this.currentMessageId,
          role: "agent",
          content: [textContent as ContentBlock],
          contentFormat: "text",
        });
      } catch {
        // Fail-safe: don't let emit errors break agent execution
      }
      
      this.currentMessageId = null;
      this.currentTextContent = "";
    }
  }
  
  /**
   * Extracts content blocks from LLM output.
   * Supports LangChain v1.0.0 AIMessageChunk content structure.
   * 
   * @param output - The LLM output
   * @returns Array of content blocks if present, empty array otherwise
   */
  private extractContentBlocks(output: any): Array<{ type: string; reasoning?: string; text?: string; [key: string]: unknown }> {
    if (!output) return [];
    
    // Check for LangChain v1.0.0 AIMessageChunk with contentBlocks or content
    const content = output.content;
    
    if (Array.isArray(content)) {
      // LangChain v1.0.0 structured content blocks
      return content.filter((block: any) => 
        block && typeof block === 'object' && block.type !== undefined
      );
    }
    
    // Check for raw AIMessageChunk with content property
    if (content?.contentBlocks && Array.isArray(content.contentBlocks)) {
      return content.contentBlocks;
    }
    
    return [];
  }
  
  /**
   * Processes content blocks and emits appropriate ACP events.
   * Emits reasoning blocks as agent_thought_chunk with audience: ['assistant'].
   * Processes blocks in their original order to preserve the intended flow.
   * 
   * @param contentBlocks - Array of content blocks from LangChain
   * @param messageId - The message ID to use for agent_message chunks
   */
  private async processContentBlocks(
    contentBlocks: Array<{ type: string; reasoning?: string; text?: string; [key: string]: unknown }>,
    messageId: string | null
  ): Promise<void> {
    // Sort blocks by index to preserve original order if index is available
    const sortedBlocks = [...contentBlocks].sort((a, b) => {
      const indexA = (a.index ?? 0) as number;
      const indexB = (b.index ?? 0) as number;
      return indexA - indexB;
    });
    
    // Track accumulated text for consecutive text blocks
    let accumulatedText = "";
    let hasAnyContent = false;
    
    for (const block of sortedBlocks) {
      if (block.type === "reasoning" && block.reasoning) {
        // Flush any accumulated text before emitting reasoning
        if (accumulatedText.length > 0 && messageId) {
          const textContent: TextContent & { type: "text" } = {
            type: "text",
            text: accumulatedText,
            _meta: null,
            annotations: null,
          };
          
          await this.connection.sendAgentMessage({
            messageId,
            role: "agent",
            content: [textContent as ContentBlock],
            contentFormat: "text",
          });
          accumulatedText = "";
        }
        
        // Emit reasoning as agent_thought_chunk
        const thoughtMessageId = this.generateMessageId();
        await this.sendAgentThoughtChunk(
          thoughtMessageId,
          block.reasoning,
          block.reasoning
        );
        hasAnyContent = true;
      } else if (block.type === "text" && block.text) {
        // Accumulate text blocks
        accumulatedText += block.text;
        hasAnyContent = true;
      }
    }
    
    // Flush remaining accumulated text
    if (accumulatedText.length > 0 && messageId) {
      const textContent: TextContent & { type: "text" } = {
        type: "text",
        text: accumulatedText,
        _meta: null,
        annotations: null,
      };
      
      await this.connection.sendAgentMessage({
        messageId,
        role: "agent",
        content: [textContent as ContentBlock],
        contentFormat: "text",
      });
    }
  }

  /**
   * Called when an LLM call encounters an error.
   * 
   * @param _error - The error that occurred
   * @param runId - The run ID for this LLM call
   * @param _parentRunId - The parent run ID if this is a nested call
   */
  override async handleLLMError(
    _error: Error,
    runId: string,
    _parentRunId?: string
  ): Promise<void> {
    if (this.currentMessageId) {
      try {
        await this.connection.sendAgentMessage({
          messageId: this.currentMessageId,
          role: "agent",
          content: [
            {
              type: "text",
              text: `Error: ${_error.message}`,
              _meta: null,
              annotations: null,
            } as ContentBlock,
          ],
          contentFormat: "text",
        });
      } catch {
        // Fail-safe: don't let emit errors break agent execution
      }
      
      this.currentMessageId = null;
      this.currentTextContent = "";
    }
  }

  /**
   * Called when a tool starts execution.
   * 
   * @param tool - The tool being executed
   * @param input - The input to the tool
   * @param runId - The run ID for this tool call
   * @param _parentRunId - The parent run ID if this is a nested call
   * @param _tags - Optional tags
   * @param _metadata - Optional metadata
   * @param _runName - Optional run name
   */
  override async handleToolStart(
    tool: any,
    input: string,
    runId: string,
    _parentRunId?: string,
    _tags?: string[],
    _metadata?: Record<string, unknown>,
    _runName?: string
  ): Promise<void> {
    // Extract tool name from various possible locations
    const toolName = tool?.name || 
                   tool?.kwargs?.name || 
                   tool?._name || 
                   tool?.func?.name || 
                   "unknown_tool";
    
    this.currentToolCallId = this.generateToolCallId();
    
    try {
      await this.sendToolCallStart(this.currentToolCallId, toolName, input);
    } catch {
      // Fail-safe: don't let emit errors break agent execution
    }
  }

  /**
   * Called when a tool call ends.
   * 
   * @param output - The output from the tool
   * @param runId - The run ID for this tool call
   * @param _parentRunId - The parent run ID if this is a nested call
   */
  override async handleToolEnd(
    output: any,
    runId: string,
    _parentRunId?: string
  ): Promise<void> {
    if (this.currentToolCallId) {
      try {
        await this.sendToolCallEnd(this.currentToolCallId, "completed", output);
      } catch {
        // Fail-safe: don't let emit errors break agent execution
      }
      
      this.currentToolCallId = null;
    }
  }

  /**
   * Called when a tool call encounters an error.
   * 
   * @param _error - The error that occurred
   * @param runId - The run ID for this tool call
   * @param _parentRunId - The parent run ID if this is a nested call
   */
  override async handleToolError(
    _error: Error,
    runId: string,
    _parentRunId?: string
  ): Promise<void> {
    if (this.currentToolCallId) {
      try {
        await this.sendToolCallEnd(
          this.currentToolCallId, 
          "failed", 
          _error.message,
          _error.message
        );
      } catch {
        // Fail-safe: don't let emit errors break agent execution
      }
      
      this.currentToolCallId = null;
    }
  }

  /**
   * Called when an agent encounters an error.
   * 
   * This method handles agent-level errors by mapping them to ACP error codes
   * and sending the error as an agent_message_chunk to the ACP client.
   * 
   * @param error - The error that occurred
   * @param runId - The run ID for this agent execution
   * @param _parentRunId - The parent run ID if this is a nested call
   */
  override async handleAgentError(
    error: Error,
    runId: string,
    _parentRunId?: string
  ): Promise<void> {
    // Map the LangChain error to an ACP error code
    const errorCode = mapLangChainError(error);
    
    // Generate a message ID for the error message
    const messageId = this.generateMessageId();
    
    // Format the error message with the ACP error code
    const errorMessage = `[Error ${errorCode}] ${error.message}`;
    
    try {
      await this.connection.sendAgentMessage({
        messageId,
        role: "agent",
        content: [
          {
            type: "text",
            text: errorMessage,
            _meta: null,
            annotations: null,
          } as ContentBlock,
        ],
        contentFormat: "text",
      });
    } catch (e) {
      // Fail-safe: don't let emit errors break agent execution
      // Log the error for debugging purposes
      console.error("ACPCallbackHandler: Failed to send agent error to client:", e);
    }
  }

  /**
   * Sends an agent message chunk to the ACP client.
   * 
   * @param messageId - Unique identifier for this message
   * @param delta - The text delta to send
   * @param currentText - The accumulated text so far
   */
  private async sendAgentMessageChunk(
    messageId: string,
    delta: string,
    currentText: string
  ): Promise<void> {
    const textContent: TextContent & { type: "text" } = {
      type: "text",
      text: delta,
      _meta: null,
      annotations: null,
    };
    
    await this.connection.sendAgentMessage({
      messageId,
      role: "agent",
      content: [textContent as ContentBlock],
      contentFormat: "text",
      delta: {
        type: "text",
        text: delta,
      },
    });
  }
  
  /**
   * Sends an agent thought chunk to the ACP client.
   * Emits reasoning content as agent_thought_chunk with audience: ['assistant'].
   * 
   * The agent_thought_chunk is a SessionUpdate variant that wraps a ContentChunk
   * containing reasoning content. The content block has audience: ['assistant']
   * annotation to indicate it's internal thought content.
   * 
   * @param messageId - Unique identifier for this thought message
   * @param delta - The text delta to send
   * @param currentText - The accumulated reasoning content so far
   */
  private async sendAgentThoughtChunk(
    messageId: string,
    delta: string,
    currentText: string
  ): Promise<void> {
    // Build the content block with audience: ['assistant'] annotation
    const textContent: TextContent & { type: "text" } = {
      type: "text",
      text: delta,
      _meta: { _internal: true, reasoning: true },
      annotations: {
        _meta: null,
        audience: ["assistant"] as const,
        lastModified: null,
        priority: null,
      },
    };
    
    // Wrap in ContentChunk for agent_thought_chunk
    const contentChunk: ContentChunk = {
      _meta: null,
      content: textContent as ContentBlock,
    };
    
    // Check if we should use agent_thought_chunk or fall back to agent_message_chunk
    // agent_thought_chunk is not yet part of the stable protocol specification
    if (!this.emitReasoningAsThought) {
      // Fallback: emit as regular agent message with audience annotation
      await this.connection.sendAgentMessage({
        messageId,
        role: "agent",
        content: [textContent as ContentBlock],
        contentFormat: "text",
        delta: {
          type: "text",
          text: delta,
        },
      });
      return;
    }
    
    // Use agent_thought_chunk with sessionUpdate
    if (!this.sessionId) {
      // Cannot emit session update without session ID, fallback to agent message
      await this.connection.sendAgentMessage({
        messageId,
        role: "agent",
        content: [textContent as ContentBlock],
        contentFormat: "text",
        delta: {
          type: "text",
          text: delta,
        },
      });
      return;
    }
    
    // Emit agent_thought_chunk via sessionUpdate
    // The update object is a SessionUpdate variant with sessionUpdate discriminator
    await this.connection.sessionUpdate({
      sessionId: this.sessionId,
      update: {
        sessionUpdate: "agent_thought_chunk",
        ...contentChunk,
      },
    });
  }
  
  /**
   * Checks if a token indicates the start of a reasoning block.
   * Detects reasoning content based on token patterns and metadata.
   * 
   * This detection is used during streaming when content blocks aren't yet formed.
   * For LangChain v1.0.0 structured content, use handleLLMEnd instead.
   * 
   * @param token - The token to check
   * @param fields - Additional fields from the LLM callback
   * @returns True if this token starts a reasoning block
   */
  private isReasoningToken(token: string, fields?: any): boolean {
    // Check fields for explicit reasoning flag (LangChain v1.0.0 metadata)
    if (fields?.reasoning === true || fields?.reasoningContent) {
      return true;
    }
    
    // Check for reasoning-specific prefixes or patterns
    // These patterns are used by models like Anthropic for reasoning output
    const reasoningPatterns = [
      /^<reasoning>/i,
      /^<thought>/i,
      /^<thinking>/i,
      /^\[reasoning\]/i,
      /^\[thought\]/i,
      /^\[thinking\]/i,
      /^<internal>/i,
      /^\(thinking\)/i,
    ];
    
    for (const pattern of reasoningPatterns) {
      if (pattern.test(token)) {
        return true;
      }
    }
    
    // Check for explicit reasoning markers at the start of content
    // Models often prefix reasoning with specific markers
    const trimmedToken = token.trim();
    const reasoningMarkers = [
      "<think>",
      "<reasoning>",
      "<thought>",
      "[reasoning]",
      "(thinking)",
    ];
    
    for (const marker of reasoningMarkers) {
      if (trimmedToken.startsWith(marker) || trimmedToken === marker) {
        return true;
      }
    }
    
    return false;
  }
  
  /**
   * Checks if a token indicates the end of a reasoning block.
   * 
   * @param token - The token to check
   * @param fields - Additional fields from the LLM callback
   * @returns True if this token ends a reasoning block
   */
  private isReasoningEndToken(token: string, fields?: any): boolean {
    // Check fields for explicit end flag
    if (fields?.reasoning === false || fields?.reasoningEnd === true) {
      return true;
    }
    
    // Check for reasoning-specific end patterns
    const reasoningEndPatterns = [
      /<\/reasoning>/i,
      /<\/thought>/i,
      /<\/thinking>/i,
      /^\[\/reasoning\]/i,
      /^\[\/thought\]/i,
      /^\[\/thinking\]/i,
      /^\]\)\s*$/,
      /^<end reasoning>/i,
    ];
    
    for (const pattern of reasoningEndPatterns) {
      if (pattern.test(token)) {
        return true;
      }
    }
    
    // Check for closing reasoning markers
    const trimmedToken = token.trim();
    const closingMarkers = [
      "<\/reasoning>",
      "<\/thought>",
      "<\/thinking>",
      "]<",
      "])",
    ];
    
    for (const marker of closingMarkers) {
      if (trimmedToken.includes(marker)) {
        return true;
      }
    }
    
    return false;
  }

  /**
   * Generates a unique message ID.
   * Uses a combination of timestamp, counter, and random suffix for uniqueness.
   */
  private generateMessageId(): string {
    const timestamp = Date.now();
    const random = Math.random().toString(36).substring(2, 8);
    const counter = (this as any)._messageCounter ?? ((this as any)._messageCounter = 0);
    (this as any)._messageCounter = counter + 1;
    return `msg-${timestamp}-${counter}-${random}`;
  }

  /**
   * Generates a unique tool call ID.
   * Uses a combination of timestamp, counter, and random suffix for uniqueness.
   */
  private generateToolCallId(): string {
    const timestamp = Date.now();
    const random = Math.random().toString(36).substring(2, 8);
    const counter = (this as any)._toolCallCounter ?? ((this as any)._toolCallCounter = 0);
    (this as any)._toolCallCounter = counter + 1;
    return `tool-${timestamp}-${counter}-${random}`;
  }

  /**
   * Detects the tool kind from the tool name.
   * Maps common tool names to appropriate ACP tool kinds.
   * 
   * @param toolName - The name of the tool
   * @returns The detected tool kind
   */
  private detectToolKind(toolName: string): "read" | "edit" | "delete" | "move" | "search" | "execute" | "think" | "fetch" | "switch_mode" | "other" {
    const name = toolName.toLowerCase();
    
    if (name.includes("read") || name.includes("file") || name.includes("load") || name.includes("get")) {
      return "read";
    }
    if (name.includes("write") || name.includes("edit") || name.includes("create") || name.includes("update") || name.includes("modify")) {
      return "edit";
    }
    if (name.includes("delete") || name.includes("remove") || name.includes("rm")) {
      return "delete";
    }
    if (name.includes("move") || name.includes("rename") || name.includes("mv")) {
      return "move";
    }
    if (name.includes("search") || name.includes("find") || name.includes("grep") || name.includes("query")) {
      return "search";
    }
    if (name.includes("exec") || name.includes("run") || name.includes("command") || name.includes("shell") || name.includes("bash") || name.includes("cmd")) {
      return "execute";
    }
    if (name.includes("think") || name.includes("reason") || name.includes("analyze")) {
      return "think";
    }
    if (name.includes("fetch") || name.includes("http") || name.includes("request") || name.includes("api") || name.includes("url")) {
      return "fetch";
    }
    if (name.includes("mode") || name.includes("switch")) {
      return "switch_mode";
    }
    
    return "other";
  }

  /**
   * Sends a session update for tool call creation.
   * 
   * @param toolCallId - Unique identifier for the tool call
   * @param toolName - Name of the tool being called
   * @param input - Input parameters for the tool
   */
  private async sendToolCallStart(
    toolCallId: string,
    toolName: string,
    input: string
  ): Promise<void> {
    if (!this.sessionId) {
      // Fallback: emit as agent message if no session ID
      await this.connection.sendAgentMessage({
        messageId: this.generateMessageId(),
        role: "agent",
        content: [
          {
            type: "text",
            text: `Calling tool: ${toolName}`,
            _meta: null,
            annotations: null,
          } as ContentBlock,
        ],
        contentFormat: "text",
      });
      return;
    }

    await this.connection.sessionUpdate({
      sessionId: this.sessionId,
      update: {
        sessionUpdate: "tool_call",
        toolCallId,
        title: `Calling tool: ${toolName}`,
        kind: this.detectToolKind(toolName),
        status: "in_progress",
        rawInput: input,
      },
    });
  }

  /**
   * Sends a session update for tool call completion.
   * 
   * @param toolCallId - Unique identifier for the tool call
   * @param status - Completion status (completed or failed)
   * @param output - Output from the tool
   * @param errorMessage - Error message if failed
   */
  private async sendToolCallEnd(
    toolCallId: string,
    status: "completed" | "failed",
    output: unknown,
    errorMessage?: string
  ): Promise<void> {
    if (!this.sessionId) {
      // Fallback: emit as agent message if no session ID
      const content = status === "completed" 
        ? `Tool completed: ${output}`
        : `Tool error: ${errorMessage || output}`;
      
      await this.connection.sendAgentMessage({
        messageId: this.generateMessageId(),
        role: "agent",
        content: [
          {
            type: "text",
            text: content,
            _meta: null,
            annotations: null,
          } as ContentBlock,
        ],
        contentFormat: "text",
      });
      return;
    }

    // Wrap content in ToolCallContent structure
    // ToolCallContent expects: { type: "content", content: ContentBlock }
    const toolCallContent = status === "completed" 
      ? [{
          type: "content" as const,
          content: {
            type: "text" as const,
            text: String(output),
            _meta: null,
            annotations: null,
          },
        }]
      : [{
          type: "content" as const,
          content: {
            type: "text" as const,
            text: errorMessage || String(output),
            _meta: null,
            annotations: null,
          },
        }];

    await this.connection.sessionUpdate({
      sessionId: this.sessionId,
      update: {
        sessionUpdate: "tool_call_update",
        toolCallId,
        status,
        content: toolCallContent,
        rawOutput: output,
        _meta: null,
      },
    });
  }
}

/**
 * Factory function for creating an ACPCallbackHandler.
 * 
 * @param config - Configuration options for the callback handler
 * @returns A new ACPCallbackHandler instance
 * 
 * @example
 * ```typescript
 * const handler = createACPCallbackHandler({
 *   connection: agentConnection,
 *   emitTextChunks: true,
 * });
 * ```
 */
export function createACPCallbackHandler(
  config: ACPCallbackHandlerConfig
): ACPCallbackHandler {
  return new ACPCallbackHandler(config);
}